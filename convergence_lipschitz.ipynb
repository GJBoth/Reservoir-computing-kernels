{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import seaborn\n",
    "import sys\n",
    "seaborn.set()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import reservoir\n",
    "import reckernel\n",
    "import kuramoto\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear activation function\n",
    "\n",
    "## $L$ smaller than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_projection = 'gaussian'  # \"gaussian\" for RC, \"structured\" for SRC\n",
    "redraw = False  # to choose if weights are redrawn at each iteration\n",
    "\n",
    "n_rep = 40\n",
    "input_len_range = np.arange(start=1, stop=10, step=1)\n",
    "n_res_range = np.logspace(2, 4, num=20, dtype=int)\n",
    "input_scale = .1  # scale of input in reservoir dynamics\n",
    "res_scale = .5  # scale of reservoir in dynamics\n",
    "n_input = 50 # number of time series, usually 50 is enough\n",
    "input_dim = 100  # dimension of the time series, larger reduces the variance of the final result\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "RK = RecKernel(function='linear', res_scale=res_scale, input_scale=input_scale, memory_efficient=False)\n",
    "\n",
    "error_stable = torch.zeros(len(input_len_range), len(n_res_range), n_rep)\n",
    "for k in range(n_rep):\n",
    "    torch.manual_seed(k)\n",
    "    for i, n_res in tqdm(enumerate(n_res_range)):\n",
    "        initial_states = torch.randn(n_input, n_res).to(device) / np.sqrt(n_res)\n",
    "        initial_K = initial_states @ initial_states.T\n",
    "        model = reservoir.ESN(input_dim, res_size=n_res, res_scale=res_scale, input_scale=input_scale, \n",
    "                              f='linear', random_projection=random_projection, seed=k, redraw=redraw)\n",
    "        for j, input_len in (enumerate(input_len_range)):\n",
    "            input_data = torch.randn(n_input, input_len, input_dim).to(device) / np.sqrt(input_dim)\n",
    "\n",
    "            K = RK.forward(input_data, initial_K=initial_K)\n",
    "            \n",
    "            X_final = torch.zeros((n_input,n_res)).to(device)\n",
    "            for n_in in range(n_input):\n",
    "                X = model.forward(input_data[n_in,:,:].reshape(input_len,input_dim), initial_state=initial_state).to(device)\n",
    "                X_final[n_in,:] = X[-1,:]\n",
    "            K_hat = torch.matmul(X_final,X_final.t())\n",
    "            \n",
    "            error_stable[j, i, k] = loss(K_hat,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "ncurves = 5\n",
    "plt.loglog(n_res_range, torch.mean(error_stable, axis=2)[:ncurves, :].T)\n",
    "plt.legend(input_len_range[:ncurves])\n",
    "plt.title('Convergence of Reservoir Computing towards Recurrent Kernels')\n",
    "plt.xlabel('Reservoir dimension')\n",
    "plt.ylabel('Mean-square error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to change the file name\n",
    "np.save('out/200521_conv_rc_lin_stable', error_stable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $L$ equal to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_projection = 'gaussian'  # \"gaussian\" for RC, \"structured\" for SRC\n",
    "redraw = False  # to choose if weights are redrawn at each iteration\n",
    "\n",
    "n_rep = 40\n",
    "input_len_range = np.arange(start=1, stop=10, step=1)\n",
    "n_res_range = np.logspace(2, 4, num=20, dtype=int)\n",
    "input_scale = .1  # scale of input in reservoir dynamics\n",
    "res_scale = 1  # scale of reservoir in dynamics\n",
    "n_input = 50 # number of time series, usually 50 is enough\n",
    "input_dim = 100  # dimension of the time series, larger reduces the variance of the final result\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "RK = RecKernel(function='linear', res_scale=res_scale, input_scale=input_scale, memory_efficient=False)\n",
    "\n",
    "error_transition = torch.zeros(len(input_len_range), len(n_res_range), n_rep)\n",
    "for k in range(n_rep):\n",
    "    torch.manual_seed(k)\n",
    "    for i, n_res in tqdm(enumerate(n_res_range)):\n",
    "        initial_states = torch.randn(n_input, n_res).to(device) / np.sqrt(n_res)\n",
    "        initial_K = initial_states @ initial_states.T\n",
    "        model = reservoir.ESN(input_dim, res_size=n_res, res_scale=res_scale, input_scale=input_scale, \n",
    "                              f='linear', random_projection=random_projection, seed=k, redraw=redraw)\n",
    "        for j, input_len in (enumerate(input_len_range)):\n",
    "            input_data = torch.randn(n_input, input_len, input_dim).to(device) / np.sqrt(input_dim)\n",
    "\n",
    "            K = RK.forward(input_data, initial_K=initial_K)\n",
    "            \n",
    "            X_final = torch.zeros((n_input,n_res)).to(device)\n",
    "            for n_in in range(n_input):\n",
    "                X = model.forward(input_data[n_in,:,:].reshape(input_len,input_dim), initial_state=initial_state).to(device)\n",
    "                X_final[n_in,:] = X[-1,:]\n",
    "            K_hat = torch.matmul(X_final,X_final.t())\n",
    "            \n",
    "            error_transition[j, i, k] = loss(K_hat,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "ncurves = 5\n",
    "plt.loglog(n_res_range, torch.mean(error_transition, axis=2)[:ncurves, :].T)\n",
    "plt.legend(input_len_range[:ncurves])\n",
    "plt.title('Convergence of Reservoir Computing towards Recurrent Kernels')\n",
    "plt.xlabel('Reservoir dimension')\n",
    "plt.ylabel('Mean-square error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to change the file name\n",
    "np.save('out/200521_conv_rc_lin_transition', error_transition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $L$ greater than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_projection = 'gaussian'  # \"gaussian\" for RC, \"structured\" for SRC\n",
    "redraw = False  # to choose if weights are redrawn at each iteration\n",
    "\n",
    "n_rep = 40\n",
    "input_len_range = np.arange(start=1, stop=10, step=1)\n",
    "n_res_range = np.logspace(2, 4, num=20, dtype=int)\n",
    "input_scale = .1  # scale of input in reservoir dynamics\n",
    "res_scale = 1  # scale of reservoir in dynamics\n",
    "n_input = 50 # number of time series, usually 50 is enough\n",
    "input_dim = 100  # dimension of the time series, larger reduces the variance of the final result\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "RK = RecKernel(function='linear', res_scale=res_scale, input_scale=input_scale, memory_efficient=False)\n",
    "\n",
    "error_chaos = torch.zeros(len(input_len_range), len(n_res_range), n_rep)\n",
    "for k in range(n_rep):\n",
    "    torch.manual_seed(k)\n",
    "    for i, n_res in tqdm(enumerate(n_res_range)):\n",
    "        initial_states = torch.randn(n_input, n_res).to(device) / np.sqrt(n_res)\n",
    "        initial_K = initial_states @ initial_states.T\n",
    "        model = reservoir.ESN(input_dim, res_size=n_res, res_scale=res_scale, input_scale=input_scale, \n",
    "                              f='linear', random_projection=random_projection, seed=k, redraw=redraw)\n",
    "        for j, input_len in (enumerate(input_len_range)):\n",
    "            input_data = torch.randn(n_input, input_len, input_dim).to(device) / np.sqrt(input_dim)\n",
    "\n",
    "            K = RK.forward(input_data, initial_K=initial_K)\n",
    "            \n",
    "            X_final = torch.zeros((n_input,n_res)).to(device)\n",
    "            for n_in in range(n_input):\n",
    "                X = model.forward(input_data[n_in,:,:].reshape(input_len,input_dim), initial_state=initial_state).to(device)\n",
    "                X_final[n_in,:] = X[-1,:]\n",
    "            K_hat = torch.matmul(X_final,X_final.t())\n",
    "            \n",
    "            error_chaos[j, i, k] = loss(K_hat,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "ncurves = 5\n",
    "plt.loglog(n_res_range, torch.mean(error_chaos, axis=2)[:ncurves, :].T)\n",
    "plt.legend(input_len_range[:ncurves])\n",
    "plt.title('Convergence of Reservoir Computing towards Recurrent Kernels')\n",
    "plt.xlabel('Reservoir dimension')\n",
    "plt.ylabel('Mean-square error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to change the file name\n",
    "np.save('out/200521_conv_rc_lin_chaos', error_chaos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arcsine kernel\n",
    "\n",
    "## $L$ smaller than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_projection = 'gaussian'  # \"gaussian\" for RC, \"structured\" for SRC\n",
    "redraw = False  # to choose if weights are redrawn at each iteration\n",
    "\n",
    "n_rep = 40\n",
    "input_len_range = np.arange(start=1, stop=10, step=1)\n",
    "n_res_range = np.logspace(2, 4, num=20, dtype=int)\n",
    "input_scale = .1  # scale of input in reservoir dynamics\n",
    "res_scale = .5  # scale of reservoir in dynamics\n",
    "n_input = 50 # number of time series, usually 50 is enough\n",
    "input_dim = 100  # dimension of the time series, larger reduces the variance of the final result\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "RK = RecKernel(function='arcsin', res_scale=res_scale, input_scale=input_scale, memory_efficient=False)\n",
    "\n",
    "error_stable = torch.zeros(len(input_len_range), len(n_res_range), n_rep)\n",
    "for k in range(n_rep):\n",
    "    torch.manual_seed(k)\n",
    "    for i, n_res in tqdm(enumerate(n_res_range)):\n",
    "        initial_states = torch.randn(n_input, n_res).to(device) / np.sqrt(n_res)\n",
    "        initial_K = initial_states @ initial_states.T\n",
    "        model = reservoir.ESN(input_dim, res_size=n_res, res_scale=res_scale, input_scale=input_scale, \n",
    "                              f='erf', random_projection=random_projection, seed=k, redraw=redraw)\n",
    "        for j, input_len in (enumerate(input_len_range)):\n",
    "            input_data = torch.randn(n_input, input_len, input_dim).to(device) / np.sqrt(input_dim)\n",
    "\n",
    "            K = RK.forward(input_data, initial_K=initial_K)\n",
    "            \n",
    "            X_final = torch.zeros((n_input,n_res)).to(device)\n",
    "            for n_in in range(n_input):\n",
    "                X = model.forward(input_data[n_in,:,:].reshape(input_len,input_dim), initial_state=initial_state).to(device)\n",
    "                X_final[n_in,:] = X[-1,:]\n",
    "            K_hat = torch.matmul(X_final,X_final.t())\n",
    "            \n",
    "            error_stable[j, i, k] = loss(K_hat,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "ncurves = 5\n",
    "plt.loglog(n_res_range, torch.mean(error_stable, axis=2)[:ncurves, :].T)\n",
    "plt.legend(input_len_range[:ncurves])\n",
    "plt.title('Convergence of Reservoir Computing towards Recurrent Kernels')\n",
    "plt.xlabel('Reservoir dimension')\n",
    "plt.ylabel('Mean-square error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to change the file name\n",
    "np.save('out/200521_conv_rc_arcsin_stable', error_stable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $L$ equal to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_projection = 'gaussian'  # \"gaussian\" for RC, \"structured\" for SRC\n",
    "redraw = False  # to choose if weights are redrawn at each iteration\n",
    "\n",
    "n_rep = 40\n",
    "input_len_range = np.arange(start=1, stop=10, step=1)\n",
    "n_res_range = np.logspace(2, 4, num=20, dtype=int)\n",
    "input_scale = .1  # scale of input in reservoir dynamics\n",
    "res_scale = 1  # scale of reservoir in dynamics\n",
    "n_input = 50 # number of time series, usually 50 is enough\n",
    "input_dim = 100  # dimension of the time series, larger reduces the variance of the final result\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "RK = RecKernel(function='arcsin', res_scale=res_scale, input_scale=input_scale, memory_efficient=False)\n",
    "\n",
    "error_transition = torch.zeros(len(input_len_range), len(n_res_range), n_rep)\n",
    "for k in range(n_rep):\n",
    "    torch.manual_seed(k)\n",
    "    for i, n_res in tqdm(enumerate(n_res_range)):\n",
    "        initial_states = torch.randn(n_input, n_res).to(device) / np.sqrt(n_res)\n",
    "        initial_K = initial_states @ initial_states.T\n",
    "        model = reservoir.ESN(input_dim, res_size=n_res, res_scale=res_scale, input_scale=input_scale, \n",
    "                              f='erf', random_projection=random_projection, seed=k, redraw=redraw)\n",
    "        for j, input_len in (enumerate(input_len_range)):\n",
    "            input_data = torch.randn(n_input, input_len, input_dim).to(device) / np.sqrt(input_dim)\n",
    "\n",
    "            K = RK.forward(input_data, initial_K=initial_K)\n",
    "            \n",
    "            X_final = torch.zeros((n_input,n_res)).to(device)\n",
    "            for n_in in range(n_input):\n",
    "                X = model.forward(input_data[n_in,:,:].reshape(input_len,input_dim), initial_state=initial_state).to(device)\n",
    "                X_final[n_in,:] = X[-1,:]\n",
    "            K_hat = torch.matmul(X_final,X_final.t())\n",
    "            \n",
    "            error_transition[j, i, k] = loss(K_hat,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "ncurves = 5\n",
    "plt.loglog(n_res_range, torch.mean(error_transition, axis=2)[:ncurves, :].T)\n",
    "plt.legend(input_len_range[:ncurves])\n",
    "plt.title('Convergence of Reservoir Computing towards Recurrent Kernels')\n",
    "plt.xlabel('Reservoir dimension')\n",
    "plt.ylabel('Mean-square error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to change the file name\n",
    "np.save('out/200521_conv_rc_arcsin_transition', error_transition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $L$ greater than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_projection = 'gaussian'  # \"gaussian\" for RC, \"structured\" for SRC\n",
    "redraw = False  # to choose if weights are redrawn at each iteration\n",
    "\n",
    "n_rep = 40\n",
    "input_len_range = np.arange(start=1, stop=10, step=1)\n",
    "n_res_range = np.logspace(2, 4, num=20, dtype=int)\n",
    "input_scale = .1  # scale of input in reservoir dynamics\n",
    "res_scale = 1  # scale of reservoir in dynamics\n",
    "n_input = 50 # number of time series, usually 50 is enough\n",
    "input_dim = 100  # dimension of the time series, larger reduces the variance of the final result\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "RK = RecKernel(function='arcsin', res_scale=res_scale, input_scale=input_scale, memory_efficient=False)\n",
    "\n",
    "error_chaos = torch.zeros(len(input_len_range), len(n_res_range), n_rep)\n",
    "for k in range(n_rep):\n",
    "    torch.manual_seed(k)\n",
    "    for i, n_res in tqdm(enumerate(n_res_range)):\n",
    "        initial_states = torch.randn(n_input, n_res).to(device) / np.sqrt(n_res)\n",
    "        initial_K = initial_states @ initial_states.T\n",
    "        model = reservoir.ESN(input_dim, res_size=n_res, res_scale=res_scale, input_scale=input_scale, \n",
    "                              f='erf', random_projection=random_projection, seed=k, redraw=redraw)\n",
    "        for j, input_len in (enumerate(input_len_range)):\n",
    "            input_data = torch.randn(n_input, input_len, input_dim).to(device) / np.sqrt(input_dim)\n",
    "\n",
    "            K = RK.forward(input_data, initial_K=initial_K)\n",
    "            \n",
    "            X_final = torch.zeros((n_input,n_res)).to(device)\n",
    "            for n_in in range(n_input):\n",
    "                X = model.forward(input_data[n_in,:,:].reshape(input_len,input_dim), initial_state=initial_state).to(device)\n",
    "                X_final[n_in,:] = X[-1,:]\n",
    "            K_hat = torch.matmul(X_final,X_final.t())\n",
    "            \n",
    "            error_chaos[j, i, k] = loss(K_hat,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "ncurves = 5\n",
    "plt.loglog(n_res_range, torch.mean(error_chaos, axis=2)[:ncurves, :].T)\n",
    "plt.legend(input_len_range[:ncurves])\n",
    "plt.title('Convergence of Reservoir Computing towards Recurrent Kernels')\n",
    "plt.xlabel('Reservoir dimension')\n",
    "plt.ylabel('Mean-square error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to change the file name\n",
    "np.save('out/200521_conv_rc_arcsin_chaos', error_chaos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear activation function\n",
    "\n",
    "## $L$ smaller than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_projection = 'gaussian'  # \"gaussian\" for RC, \"structured\" for SRC\n",
    "redraw = False  # to choose if weights are redrawn at each iteration\n",
    "\n",
    "n_rep = 40\n",
    "input_len_range = np.arange(start=1, stop=10, step=1)\n",
    "n_res_range = np.logspace(2, 4, num=20, dtype=int)\n",
    "input_scale = .1  # scale of input in reservoir dynamics\n",
    "res_scale = .5  # scale of reservoir in dynamics\n",
    "n_input = 50 # number of time series, usually 50 is enough\n",
    "input_dim = 100  # dimension of the time series, larger reduces the variance of the final result\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "RK = RecKernel(function='rbf', res_scale=res_scale, input_scale=input_scale, memory_efficient=False)\n",
    "\n",
    "error_stable = torch.zeros(len(input_len_range), len(n_res_range), n_rep)\n",
    "for k in range(n_rep):\n",
    "    torch.manual_seed(k)\n",
    "    for i, n_res in tqdm(enumerate(n_res_range)):\n",
    "        initial_states = torch.randn(n_input, n_res).to(device) / np.sqrt(n_res)\n",
    "        initial_K = initial_states @ initial_states.T\n",
    "        model = reservoir.ESN(input_dim, res_size=n_res, res_scale=res_scale, input_scale=input_scale, \n",
    "                              f='cos_rbf', random_projection=random_projection, seed=k, redraw=redraw)\n",
    "        for j, input_len in (enumerate(input_len_range)):\n",
    "            input_data = torch.randn(n_input, input_len, input_dim).to(device) / np.sqrt(input_dim)\n",
    "\n",
    "            K = RK.forward(input_data, initial_K=initial_K)\n",
    "            \n",
    "            X_final = torch.zeros((n_input,n_res)).to(device)\n",
    "            for n_in in range(n_input):\n",
    "                X = model.forward(input_data[n_in,:,:].reshape(input_len,input_dim), initial_state=initial_state).to(device)\n",
    "                X_final[n_in,:] = X[-1,:]\n",
    "            K_hat = torch.matmul(X_final,X_final.t())\n",
    "            \n",
    "            error_stable[j, i, k] = loss(K_hat,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "ncurves = 5\n",
    "plt.loglog(n_res_range, torch.mean(error_stable, axis=2)[:ncurves, :].T)\n",
    "plt.legend(input_len_range[:ncurves])\n",
    "plt.title('Convergence of Reservoir Computing towards Recurrent Kernels')\n",
    "plt.xlabel('Reservoir dimension')\n",
    "plt.ylabel('Mean-square error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to change the file name\n",
    "np.save('out/200521_conv_rc_rbf_stable', error_stable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $L$ equal to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_projection = 'gaussian'  # \"gaussian\" for RC, \"structured\" for SRC\n",
    "redraw = False  # to choose if weights are redrawn at each iteration\n",
    "\n",
    "n_rep = 40\n",
    "input_len_range = np.arange(start=1, stop=10, step=1)\n",
    "n_res_range = np.logspace(2, 4, num=20, dtype=int)\n",
    "input_scale = .1  # scale of input in reservoir dynamics\n",
    "res_scale = 1  # scale of reservoir in dynamics\n",
    "n_input = 50 # number of time series, usually 50 is enough\n",
    "input_dim = 100  # dimension of the time series, larger reduces the variance of the final result\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "RK = RecKernel(function='rbf', res_scale=res_scale, input_scale=input_scale, memory_efficient=False)\n",
    "\n",
    "error_transition = torch.zeros(len(input_len_range), len(n_res_range), n_rep)\n",
    "for k in range(n_rep):\n",
    "    torch.manual_seed(k)\n",
    "    for i, n_res in tqdm(enumerate(n_res_range)):\n",
    "        initial_states = torch.randn(n_input, n_res).to(device) / np.sqrt(n_res)\n",
    "        initial_K = initial_states @ initial_states.T\n",
    "        model = reservoir.ESN(input_dim, res_size=n_res, res_scale=res_scale, input_scale=input_scale, \n",
    "                              f='cos_rbf', random_projection=random_projection, seed=k, redraw=redraw)\n",
    "        for j, input_len in (enumerate(input_len_range)):\n",
    "            input_data = torch.randn(n_input, input_len, input_dim).to(device) / np.sqrt(input_dim)\n",
    "\n",
    "            K = RK.forward(input_data, initial_K=initial_K)\n",
    "            \n",
    "            X_final = torch.zeros((n_input,n_res)).to(device)\n",
    "            for n_in in range(n_input):\n",
    "                X = model.forward(input_data[n_in,:,:].reshape(input_len,input_dim), initial_state=initial_state).to(device)\n",
    "                X_final[n_in,:] = X[-1,:]\n",
    "            K_hat = torch.matmul(X_final,X_final.t())\n",
    "            \n",
    "            error_transition[j, i, k] = loss(K_hat,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "ncurves = 5\n",
    "plt.loglog(n_res_range, torch.mean(error_transition, axis=2)[:ncurves, :].T)\n",
    "plt.legend(input_len_range[:ncurves])\n",
    "plt.title('Convergence of Reservoir Computing towards Recurrent Kernels')\n",
    "plt.xlabel('Reservoir dimension')\n",
    "plt.ylabel('Mean-square error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to change the file name\n",
    "np.save('out/200521_conv_rc_rbf_transition', error_transition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $L$ greater than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_projection = 'gaussian'  # \"gaussian\" for RC, \"structured\" for SRC\n",
    "redraw = False  # to choose if weights are redrawn at each iteration\n",
    "\n",
    "n_rep = 40\n",
    "input_len_range = np.arange(start=1, stop=10, step=1)\n",
    "n_res_range = np.logspace(2, 4, num=20, dtype=int)\n",
    "input_scale = .1  # scale of input in reservoir dynamics\n",
    "res_scale = 1  # scale of reservoir in dynamics\n",
    "n_input = 50 # number of time series, usually 50 is enough\n",
    "input_dim = 100  # dimension of the time series, larger reduces the variance of the final result\n",
    "\n",
    "loss = torch.nn.MSELoss()\n",
    "RK = RecKernel(function='rbf', res_scale=res_scale, input_scale=input_scale, memory_efficient=False)\n",
    "\n",
    "error_chaos = torch.zeros(len(input_len_range), len(n_res_range), n_rep)\n",
    "for k in range(n_rep):\n",
    "    torch.manual_seed(k)\n",
    "    for i, n_res in tqdm(enumerate(n_res_range)):\n",
    "        initial_states = torch.randn(n_input, n_res).to(device) / np.sqrt(n_res)\n",
    "        initial_K = initial_states @ initial_states.T\n",
    "        model = reservoir.ESN(input_dim, res_size=n_res, res_scale=res_scale, input_scale=input_scale, \n",
    "                              f='cos_rbf', random_projection=random_projection, seed=k, redraw=redraw)\n",
    "        for j, input_len in (enumerate(input_len_range)):\n",
    "            input_data = torch.randn(n_input, input_len, input_dim).to(device) / np.sqrt(input_dim)\n",
    "\n",
    "            K = RK.forward(input_data, initial_K=initial_K)\n",
    "            \n",
    "            X_final = torch.zeros((n_input,n_res)).to(device)\n",
    "            for n_in in range(n_input):\n",
    "                X = model.forward(input_data[n_in,:,:].reshape(input_len,input_dim), initial_state=initial_state).to(device)\n",
    "                X_final[n_in,:] = X[-1,:]\n",
    "            K_hat = torch.matmul(X_final,X_final.t())\n",
    "            \n",
    "            error_chaos[j, i, k] = loss(K_hat,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots\n",
    "ncurves = 5\n",
    "plt.loglog(n_res_range, torch.mean(error_chaos, axis=2)[:ncurves, :].T)\n",
    "plt.legend(input_len_range[:ncurves])\n",
    "plt.title('Convergence of Reservoir Computing towards Recurrent Kernels')\n",
    "plt.xlabel('Reservoir dimension')\n",
    "plt.ylabel('Mean-square error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to change the file name\n",
    "np.save('out/200521_conv_rc_rbf_chaos', error_chaos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
